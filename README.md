# WeWork Documentation Crawler

这是一个自动爬取企业微信API文档的工具，使用GitHub Actions定期运行。

## 📋 功能特性

- **自动化爬取**: 每小时自动运行，持续收集文档
- **智能处理**: 遇到验证码或限制时自动停止，下次运行继续
- **MDX格式**: 将HTML文档转换为MDX格式，便于阅读和处理
- **增量更新**: 只下载未存在的文档，避免重复工作
- **详细日志**: 完整的运行日志和统计信息

## 🚀 使用方法

### 自动运行
- **定时任务**: 每小时自动运行一次
- **手动触发**: 在GitHub仓库的Actions页面手动运行

### 本地运行
```bash
# 安装依赖
pip install -r requirements.txt

# 运行爬虫
python wework_doc_crawler.py
```

## 📁 文件结构

```
.
├── wework_doc_crawler.py    # 主爬虫脚本
├── requirements.txt         # Python依赖
├── .github/workflows/       # GitHub Actions配置
│   └── crawler.yml         # 爬虫工作流
├── docs/                   # 爬取的文档目录
└── README.md              # 说明文档
```

## 🔧 工作原理

1. **文档发现**: 从企业微信开发者文档主页提取文档树结构
2. **内容获取**: 使用官方API获取每个文档的内容
3. **格式转换**: 将HTML内容转换为MDX格式
4. **文件组织**: 按照文档层级结构组织文件
5. **版本控制**: 自动提交新文档到Git仓库

## ⚠️ 注意事项

- 爬虫在遇到验证码或达到请求限制时会自动停止
- 这是正常行为，下次定时任务会自动继续
- 完整的文档收集可能需要数天时间
- 所有新增文档会自动提交到仓库

## 📊 监控信息

每次运行后，GitHub Actions会提供详细的执行报告，包括：
- 新增文档数量
- 总文档数量
- 运行状态
- 错误日志（如有）

## 🛠️ 配置选项

爬虫脚本支持以下配置：
- `base_url`: 企业微信API基础URL
- `output_dir`: 文档输出目录（默认: `docs`）

## 📜 许可证

本项目仅用于学习和研究目的。请遵守企业微信官方的使用条款和robots.txt规定。
